{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7acf6d",
   "metadata": {},
   "source": [
    "# Challenge: Edit MNIST challenge images to be correctly predicted\n",
    "\n",
    "Goal: Edit the images placed in `data/MNIST/challenge` so that the provided model predicts the correct label while keeping more than 60% of the original pixels unchanged.\n",
    "\n",
    "Description: You are given a pre-trained `SmallCNN` model and a small set of challenge images. Your task is to minimally modify these images so the model classifies them correctly. This exercise encourages you to: \n",
    "- Explore the sample dataset in `data/MNIST/sample` to understand variation and typical inputs.\n",
    "- Use explainable AI (XAI) techniques (saliency maps, Grad-CAM, Integrated Gradients, occlusion, etc.) to discover what parts of the image the model relies on.\n",
    "- Propose minimal edits (pixel changes, small masks, subtle color shifts) that change model prediction while preserving at least 60% of the original pixels.\n",
    "\n",
    "Deliverables: For each edited image, save the modified image to `data/MNIST/challenge/edited/` alongside a short report (less than 2 pages) describing the XAI insights you used and the percentage of pixels preserved. For ease of use, you have the images already in that folder and you can directly work on them. You are allowed to use any external program you want to modify the image (i.e., paint, photoshop, figma, ...).\n",
    "\n",
    "The practice can be done by more than 1 person. Final grade would depend on the number of images correctly edited (n_images_correct) with their corresponding report and number of persons working together (n_persons) following the next formula:\n",
    "$$grade = 2,5 \\times n\\_images\\_correct - 2,5 * (n\\_persons - 1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallCNN definition (must match the trained model architecture)\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(12*7*7, 12)\n",
    "        self.fc2 = nn.Linear(12, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))  # 14x14\n",
    "        x = self.pool(x)                      # 7x7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.nn.functional.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# convenience transform\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8698f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path('../models')\n",
    "model_name = models_dir / 'small_cnn.pth'\n",
    "model = torch.load(model_name, weights_only=False)\n",
    "model.to(device) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b91e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images in data/MNIST/challenge/ and plot them. Label of the image is the last character of the filename.\n",
    "challenge_dir = Path('../data/MNIST/challenge')\n",
    "image_files = list(challenge_dir.glob('*.png'))\n",
    "fig, axes = plt.subplots(1, len(image_files), figsize=(12,4))\n",
    "for ax, img_file in zip(axes, image_files):\n",
    "    img = to_tensor(Image.open(img_file)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    ax.imshow(to_pil(img.squeeze().cpu()))\n",
    "    ax.set_title(f'Pred: {pred}, label: {img_file.stem[-1]}')\n",
    "    ax.axis('off')\n",
    "    print(f'{img_file.name}: Pred: {pred}, Label: {img_file.stem[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc11179",
   "metadata": {},
   "source": [
    "---\n",
    "### Any code you want to add, put it below this markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bb1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d670384a",
   "metadata": {},
   "source": [
    "---\n",
    "## Check if you have passed the challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of editing an image\n",
    "def calculate_edit_percentage(original_img, edited_img):\n",
    "    original_pixels = original_img.load()\n",
    "    edited_pixels = edited_img.load()\n",
    "    width, height = original_img.size\n",
    "    total_pixels = width * height\n",
    "    changed_pixels = 0\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if original_pixels[x, y] != edited_pixels[x, y]:\n",
    "                changed_pixels += 1\n",
    "\n",
    "    return (changed_pixels / total_pixels) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edited directory\n",
    "edited_dir = challenge_dir / 'edited'\n",
    "\n",
    "# Load edited images, check that they are predicted correctly and calculate edit percentages\n",
    "for original_img_file, edited_img_file in zip(challenge_dir.glob('*.png'), edited_dir.glob('*.png')):\n",
    "    original_img = Image.open(original_img_file)\n",
    "    edited_img = Image.open(edited_img_file)\n",
    "    # Convert the edited image to RGB if it's not\n",
    "    if edited_img.mode != 'RGB':\n",
    "        edited_img = edited_img.convert('RGB')\n",
    "\n",
    "    # Check prediction\n",
    "    img_tensor = to_tensor(edited_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    \n",
    "    print(f'Edited {edited_img_file.name}: Pred: {pred}, Label: {original_img_file.stem[-1]}, correct: {pred == int(original_img_file.stem[-1])}')\n",
    "\n",
    "    # Calculate edit percentage\n",
    "    edit_percentage = calculate_edit_percentage(original_img, edited_img)\n",
    "    print(f'Edit Percentage: {edit_percentage:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Laboratory 02 - Computer Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
